#/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ DeepL API
–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
–ù–∞ –æ—Å–Ω–æ–≤–µ deepl_simple_translator_simple.py —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏
"""

import deepl
import os
import sys
import time
import json
import hashlib
from pathlib import Path
from typing import Optional, Dict, List, Tuple
from datetime import datetime
from dotenv import load_dotenv
from enum import Enum

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
BASE_DIR = Path(__file__).resolve().parent
load_dotenv(BASE_DIR / '.env')

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DEEPL_API_KEY = os.getenv('DEEPL_API_KEY', '')
SUPPORTED_EXTENSIONS = ['.docx', '.pdf', '.html', '.htm', '.txt']
TRANSLATION_SUFFIX = "_translated"
PAUSE_BETWEEN_REQUESTS = 0.5  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
CACHE_DIR = BASE_DIR / '.translation_cache'
MAX_RETRIES = 3
RETRY_DELAY = 2.0

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ python-docx
DOCX_AVAILABLE = False
try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    pass

# –Ø–∑—ã–∫–æ–≤—ã–µ –æ–ø—Ü–∏–∏ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)
LANGUAGE_OPTIONS = {
    "1": {"name": "–†—É—Å—Å–∫–∏–π -> –ê–Ω–≥–ª–∏–π—Å–∫–∏–π (US)", "source": "RU", "target": "EN-US"},
    "2": {"name": "–†—É—Å—Å–∫–∏–π -> –ê–Ω–≥–ª–∏–π—Å–∫–∏–π (GB)", "source": "RU", "target": "EN-GB"},
    "3": {"name": "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π -> –†—É—Å—Å–∫–∏–π", "source": "EN", "target": "RU"},
    "4": {"name": "–ù–µ–º–µ—Ü–∫–∏–π -> –†—É—Å—Å–∫–∏–π", "source": "DE", "target": "RU"},
    "5": {"name": "–§—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π -> –†—É—Å—Å–∫–∏–π", "source": "FR", "target": "RU"},
    "6": {"name": "–ò—Å–ø–∞–Ω—Å–∫–∏–π -> –†—É—Å—Å–∫–∏–π", "source": "ES", "target": "RU"},
    "7": {"name": "–ò—Ç–∞–ª—å—è–Ω—Å–∫–∏–π -> –†—É—Å—Å–∫–∏–π", "source": "IT", "target": "RU"},
    "8": {"name": "–ö–∏—Ç–∞–π—Å–∫–∏–π -> –†—É—Å—Å–∫–∏–π", "source": "ZH", "target": "RU"},
    "9": {"name": "–Ø–ø–æ–Ω—Å–∫–∏–π -> –†—É—Å—Å–∫–∏–π", "source": "JA", "target": "RU"},
}

# –ù–∞—É—á–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è –≥–ª–æ—Å—Å–∞—Ä–∏–µ–≤
SCIENTIFIC_FIELDS = {
    "general": "–û–±—â–∏–µ –Ω–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã",
    "biomedicine": "–ë–∏–æ–º–µ–¥–∏—Ü–∏–Ω–∞",
    "physics": "–§–∏–∑–∏–∫–∞",
    "chemistry": "–•–∏–º–∏—è",
    "mathematics": "–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞",
    "computer_science": "–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞",
    "engineering": "–ò–Ω–∂–µ–Ω–µ—Ä–∏—è"
}

# –ü—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –≥–ª–æ—Å—Å–∞—Ä–∏–∏
SCIENTIFIC_GLOSSARIES = {
    "biomedicine": {
        "EN": {
            "hypercortisolism": "–≥–∏–ø–µ—Ä–∫–æ—Ä—Ç–∏–∑–æ–ª–∏–∑–º",
            "adrenocortical carcinoma": "–∞–¥—Ä–µ–Ω–æ–∫–æ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è –∫–∞—Ä—Ü–∏–Ω–æ–º–∞",
            "Cushing's syndrome": "—Å–∏–Ω–¥—Ä–æ–º –ö—É—à–∏–Ω–≥–∞",
            "glucocorticosteroid": "–≥–ª—é–∫–æ–∫–æ—Ä—Ç–∏–∫–æ—Å—Ç–µ—Ä–æ–∏–¥",
            "reticular zone": "—Ä–µ—Ç–∏–∫—É–ª—è—Ä–Ω–∞—è –∑–æ–Ω–∞",
            "in vitro": "in vitro",
            "in vivo": "in vivo",
            "apoptosis": "–∞–ø–æ–ø—Ç–æ–∑",
            "mitochondria": "–º–∏—Ç–æ—Ö–æ–Ω–¥—Ä–∏–∏",
            "cytokine": "—Ü–∏—Ç–æ–∫–∏–Ω"
        }
    },
    "physics": {
        "EN": {
            "quantum entanglement": "–∫–≤–∞–Ω—Ç–æ–≤–∞—è –∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç—å",
            "wave function": "–≤–æ–ª–Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è",
            "uncertainty principle": "–ø—Ä–∏–Ω—Ü–∏–ø –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏",
            "superposition": "—Å—É–ø–µ—Ä–ø–æ–∑–∏—Ü–∏—è",
            "quantum tunneling": "–∫–≤–∞–Ω—Ç–æ–≤–æ–µ —Ç—É–Ω–Ω–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ",
            "Schr√∂dinger equation": "—É—Ä–∞–≤–Ω–µ–Ω–∏–µ –®—Ä—ë–¥–∏–Ω–≥–µ—Ä–∞",
            "eigenvalue": "—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ",
            "Hamiltonian": "–≥–∞–º–∏–ª—å—Ç–æ–Ω–∏–∞–Ω"
        }
    },
    "chemistry": {
        "EN": {
            "covalent bond": "–∫–æ–≤–∞–ª–µ–Ω—Ç–Ω–∞—è —Å–≤—è–∑—å",
            "oxidation state": "—Å—Ç–µ–ø–µ–Ω—å –æ–∫–∏—Å–ª–µ–Ω–∏—è",
            "stoichiometry": "—Å—Ç–µ—Ö–∏–æ–º–µ—Ç—Ä–∏—è",
            "catalyst": "–∫–∞—Ç–∞–ª–∏–∑–∞—Ç–æ—Ä",
            "equilibrium": "—Ä–∞–≤–Ω–æ–≤–µ—Å–∏–µ",
            "enthalpy": "—ç–Ω—Ç–∞–ª—å–ø–∏—è",
            "entropy": "—ç–Ω—Ç—Ä–æ–ø–∏—è"
        }
    },
    "general": {
        "EN": {
            "et al.": "–∏ –¥—Ä.",
            "p-value": "p-–∑–Ω–∞—á–µ–Ω–∏–µ",
            "confidence interval": "–¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª",
            "standard deviation": "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ",
            "hypothesis": "–≥–∏–ø–æ—Ç–µ–∑–∞",
            "methodology": "–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è",
            "correlation": "–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è"
        }
    }
}

# –¢–æ–Ω–∞ –¥–ª—è Write API
TONE_OPTIONS = {
    "confident": "–£–≤–µ—Ä–µ–Ω–Ω—ã–π (–¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –≤—ã–≤–æ–¥–æ–≤)",
    "diplomatic": "–î–∏–ø–ª–æ–º–∞—Ç–∏—á–Ω—ã–π (–¥–ª—è –¥–∏—Å–∫—É—Å—Å–∏–π)",
    "friendly": "–î—Ä—É–∂–µ–ª—é–±–Ω—ã–π (–¥–ª—è –≤–≤–µ–¥–µ–Ω–∏—è)",
    "enthusiastic": "–≠–Ω—Ç—É–∑–∏–∞—Å—Ç–∏—á–Ω—ã–π (–¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π)"
}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞
QUALITY_SETTINGS = {
    "1": {
        "name": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (–±—ã—Å—Ç—Ä–æ)",
        "formality": "default",
        "preserve_formatting": True,
        "tag_handling": "xml",
        "use_write_api": False
    },
    "2": {
        "name": "–í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (—Ñ–æ—Ä–º–∞–ª—å–Ω–æ)",
        "formality": "more",
        "preserve_formatting": True,
        "tag_handling": "xml",
        "use_write_api": False
    },
    "3": {
        "name": "–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π —Å—Ç–∏–ª—å (Pro)",
        "formality": "more",
        "preserve_formatting": True,
        "tag_handling": "xml",
        "use_write_api": True,
        "writing_style": "academic"
    },
    "4": {
        "name": "–ë–∏–∑–Ω–µ—Å —Å—Ç–∏–ª—å",
        "formality": "more",
        "preserve_formatting": True,
        "tag_handling": "xml",
        "use_write_api": True,
        "writing_style": "business"
    },
    "5": {
        "name": "–ü—Ä–æ—Å—Ç–æ–π —Å—Ç–∏–ª—å",
        "formality": "default",
        "preserve_formatting": True,
        "tag_handling": "xml",
        "use_write_api": True,
        "writing_style": "simple"
    }
}


class TranslationCache:
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""

    def __init__(self, cache_dir: Path = CACHE_DIR):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_file = self.cache_dir / 'translations.json'
        self.cache = self._load_cache()
        self.stats_file = self.cache_dir / 'stats.json'
        self.stats = self._load_stats()

    def _load_cache(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if self.cache_file.exists():
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à: {e}")
        return {}

    def _load_stats(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        try:
            if self.stats_file.exists():
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except:
            pass
        return {"cache_hits": 0, "cache_misses": 0, "total_chars_saved": 0}

    def _save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ –≤ —Ñ–∞–π–ª"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à: {e}")

    def _save_stats(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        try:
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, indent=2)
        except:
            pass

    def get_hash(self, text: str, source_lang: str, target_lang: str,
                 formality: str = "default") -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ö—ç—à–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        key = f"{text[:1000]}|{source_lang}|{target_lang}|{formality}"
        return hashlib.md5(key.encode()).hexdigest()

    def get(self, text: str, source_lang: str, target_lang: str,
            formality: str = "default") -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏–∑ –∫—ç—à–∞"""
        hash_key = self.get_hash(text, source_lang, target_lang, formality)
        result = self.cache.get(hash_key)

        if result:
            self.stats["cache_hits"] += 1
            self.stats["total_chars_saved"] += len(text)
            self._save_stats()
        else:
            self.stats["cache_misses"] += 1
            self._save_stats()

        return result

    def set(self, text: str, translated_text: str, source_lang: str,
            target_lang: str, formality: str = "default"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ –∫—ç—à"""
        hash_key = self.get_hash(text, source_lang, target_lang, formality)
        self.cache[hash_key] = {
            "translation": translated_text,
            "timestamp": datetime.now().isoformat(),
            "source_preview": text[:100]
        }
        self._save_cache()

    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞"""
        return self.stats

    def clear(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        self.cache = {}
        self._save_cache()
        print("–ö—ç—à –æ—á–∏—â–µ–Ω.")


class GlossaryManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è–º–∏ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏"""

    def __init__(self, translator):
        self.translator = translator
        self.glossaries_dir = BASE_DIR / 'glossaries'
        self.glossaries_dir.mkdir(exist_ok=True)
        self.glossaries = {}

    def create_glossary(self, name: str, source_lang: str, target_lang: str,
                        entries: Dict[str, str]) -> Optional[str]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É –≥–ª–æ—Å—Å–∞—Ä–∏–µ–≤
            if not hasattr(self.translator, 'create_glossary'):
                print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ì–ª–æ—Å—Å–∞—Ä–∏–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ API")
                return None

            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –¥–ª—è DeepL
            entries_str = '\n'.join([f"{k}\t{v}" for k, v in entries.items()])

            glossary = self.translator.create_glossary(
                name=name,
                source_lang=source_lang,
                target_lang=target_lang,
                entries=entries_str,
                entries_format="tsv"
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –∫–æ–ø–∏—é
            glossary_file = self.glossaries_dir / f"{name}.json"
            with open(glossary_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "id": glossary.glossary_id,
                    "name": name,
                    "source_lang": source_lang,
                    "target_lang": target_lang,
                    "entries": entries,
                    "created": datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)

            self.glossaries[name] = glossary.glossary_id
            print(f"‚úì –ì–ª–æ—Å—Å–∞—Ä–∏–π '{name}' —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            return glossary.glossary_id

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥–ª–æ—Å—Å–∞—Ä–∏—è: {e}")
            return None

    def load_glossary_from_file(self, file_path: Path) -> Optional[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≥–ª–æ—Å—Å–∞—Ä–∏—è –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                if file_path.suffix == '.json':
                    return json.load(f)
                elif file_path.suffix in ['.txt', '.tsv']:
                    entries = {}
                    for line in f:
                        parts = line.strip().split('\t')
                        if len(parts) == 2:
                            entries[parts[0]] = parts[1]
                    return entries
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥–ª–æ—Å—Å–∞—Ä–∏—è: {e}")
        return None

    def list_glossaries(self) -> List[Dict]:
        """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≥–ª–æ—Å—Å–∞—Ä–∏–µ–≤"""
        glossaries = []
        for file in self.glossaries_dir.glob("*.json"):
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    glossaries.append(data)
            except:
                continue
        return glossaries

    def create_scientific_glossary(self, field: str, source_lang: str = "EN",
                                   target_lang: str = "RU") -> Optional[str]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –≥–ª–æ—Å—Å–∞—Ä–∏—è"""
        if field not in SCIENTIFIC_GLOSSARIES:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –û–±–ª–∞—Å—Ç—å '{field}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –≥–ª–æ—Å—Å–∞—Ä–∏—è—Ö")
            return None

        if source_lang not in SCIENTIFIC_GLOSSARIES[field]:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ø–∑—ã–∫ {source_lang} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è –æ–±–ª–∞—Å—Ç–∏ {field}")
            return None

        entries = SCIENTIFIC_GLOSSARIES[field][source_lang]
        glossary_name = f"Scientific_{field}_{source_lang}_{target_lang}"

        print(f"–°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—É—á–Ω–æ–≥–æ –≥–ª–æ—Å—Å–∞—Ä–∏—è: {SCIENTIFIC_FIELDS[field]}")
        print(f"–¢–µ—Ä–º–∏–Ω–æ–≤: {len(entries)}")

        return self.create_glossary(glossary_name, source_lang, target_lang, entries)


class EnhancedTranslator:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏"""

    def __init__(self):
        self.translator = None
        self.cache = TranslationCache()
        self.glossary_manager = None
        self.write_api_available = False
        self.usage_stats = {
            "documents_translated": 0,
            "characters_processed": 0,
            "errors_encountered": 0
        }

    def check_api_key(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è API –∫–ª—é—á–∞"""
        if not DEEPL_API_KEY:
            print("–û—à–∏–±–∫–∞: –ö–ª—é—á DeepL API –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            print("–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env –∏ –¥–æ–±–∞–≤—å—Ç–µ:")
            print("DEEPL_API_KEY=your_api_key_here")
            return False
        return True

    def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π"""
        print("\n–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DeepL...")

        if not self.check_api_key():
            return False

        try:
            self.translator = deepl.Translator(DEEPL_API_KEY)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤
            usage = self.translator.get_usage()
            if usage.character.count and usage.character.limit:
                remaining = usage.character.limit - usage.character.count
                percentage = (usage.character.count / usage.character.limit) * 100
                print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {usage.character.count:,} / {usage.character.limit:,} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"–û—Å—Ç–∞–ª–æ—Å—å: {remaining:,} —Å–∏–º–≤–æ–ª–æ–≤ ({100-percentage:.1f}%)")

                if remaining <= 0:
                    print("–û—à–∏–±–∫–∞: –õ–∏–º–∏—Ç —Å–∏–º–≤–æ–ª–æ–≤ –∏—Å—á–µ—Ä–ø–∞–Ω!")
                    return False
                elif remaining < 10000:
                    print("‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –û—Å—Ç–∞–ª–æ—Å—å –º–∞–ª–æ —Å–∏–º–≤–æ–ª–æ–≤!")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Write API
            self._check_write_api_availability()

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≥–ª–æ—Å—Å–∞—Ä–∏–µ–≤
            self.glossary_manager = GlossaryManager(self.translator)

            # –ü–æ–∫–∞–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞
            cache_stats = self.cache.get_stats()
            if cache_stats["cache_hits"] > 0:
                print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞:")
                print(f"  –ü–æ–ø–∞–¥–∞–Ω–∏—è: {cache_stats['cache_hits']}")
                print(f"  –ü—Ä–æ–º–∞—Ö–∏: {cache_stats['cache_misses']}")
                print(f"  –°—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤: {cache_stats['total_chars_saved']:,}")

            print("\n‚úì –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞.\n")
            return True

        except deepl.AuthorizationException:
            print("–û—à–∏–±–∫–∞: –ù–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á.")
            return False
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            return False

    def _check_write_api_availability(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Write API"""
        try:
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Write API (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ)
            if hasattr(self.translator, 'rephrase_text'):
                # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                result = self.translator.rephrase_text(
                    "Test",
                    target_lang="EN-US",
                    style="default"
                )
                self.write_api_available = True
                print("‚úì Write API –¥–æ—Å—Ç—É–ø–µ–Ω (Pro –∞–∫–∫–∞—É–Ω—Ç)")
            else:
                print("‚Ñπ Write API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (—Ç—Ä–µ–±—É–µ—Ç—Å—è Pro –ø–æ–¥–ø–∏—Å–∫–∞)")
        except:
            self.write_api_available = False
            print("‚Ñπ Write API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

    def translate_text_with_retry(self, text: str, source_lang: str,
                                  target_lang: str, **kwargs) -> Optional[str]:
        """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        for attempt in range(MAX_RETRIES):
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
                cached = self.cache.get(
                    text, source_lang, target_lang,
                    kwargs.get('formality', 'default')
                )

                if cached:
                    print("  üìã –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∫—ç—à")
                    return cached

                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥
                result = self.translator.translate_text(
                    text,
                    source_lang=source_lang,
                    target_lang=target_lang,
                    **kwargs
                )

                translated = result.text if hasattr(result, 'text') else str(result)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                self.cache.set(
                    text, translated, source_lang, target_lang,
                    kwargs.get('formality', 'default')
                )

                return translated

            except deepl.QuotaExceededException:
                print("‚úó –ü—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ API")
                break
            except deepl.TooManyRequestsException:
                wait_time = RETRY_DELAY * (attempt + 1)
                print(f"  ‚è± –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤. –û–∂–∏–¥–∞–Ω–∏–µ {wait_time} —Å–µ–∫...")
                time.sleep(wait_time)
            except Exception as e:
                if attempt < MAX_RETRIES - 1:
                    print(f"  ‚ö† –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{MAX_RETRIES} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                    time.sleep(RETRY_DELAY)
                else:
                    print(f"‚úó –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
                    self.usage_stats["errors_encountered"] += 1
                    break

        return None

    def apply_write_api_improvement(self, text: str, target_lang: str,
                                   style: str = "academic") -> str:
        """–£–ª—É—á—à–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Write API (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)"""
        if not self.write_api_available:
            return text

        try:
            if hasattr(self.translator, 'rephrase_text'):
                result = self.translator.rephrase_text(
                    text,
                    target_lang=target_lang,
                    style=style
                )
                return result.text if hasattr(result, 'text') else str(result)
        except Exception as e:
            print(f"  ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏–µ —Å—Ç–∏–ª—è: {e}")

        return text

    def translate_document_enhanced(self, file_path: Path, output_path: Path,
                                   source_lang: str, target_lang: str,
                                   quality_setting: Dict,
                                   glossary_id: Optional[str] = None) -> bool:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ–ø—Ü–∏—è–º–∏"""

        print(f"\n{'='*60}")
        print(f"üìÑ –ü–µ—Ä–µ–≤–æ–¥: {file_path.name}")
        print(f"üîÑ –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {source_lang} -> {target_lang}")
        print(f"‚öô –ù–∞—Å—Ç—Ä–æ–π–∫–∏: {quality_setting['name']}")
        if glossary_id:
            print(f"üìö –ì–ª–æ—Å—Å–∞—Ä–∏–π –ø—Ä–∏–º–µ–Ω–µ–Ω")
        print(f"{'='*60}\n")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        if output_path.exists():
            overwrite = input(f"\n–§–∞–π–ª {output_path.name} —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å? (y/n): ")
            if overwrite.lower() != 'y':
                print("–ü—Ä–æ–ø—É—Å–∫ —Ñ–∞–π–ª–∞.")
                return False

        try:
            start_time = time.time()
            file_size = file_path.stat().st_size

            print(f"üìè –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:,} –±–∞–π—Ç")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–∞
            translation_params = {
                "input_path": str(file_path),
                "output_path": str(output_path),
                "source_lang": source_lang,
                "target_lang": target_lang
            }

            # –î–æ–±–∞–≤–ª—è–µ–º formality –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
            formality_supported_langs = ['DE', 'FR', 'IT', 'ES', 'NL', 'PL', 'PT-PT', 'PT-BR', 'RU', 'JA']
            if target_lang in formality_supported_langs and quality_setting.get('formality') != 'default':
                translation_params["formality"] = quality_setting['formality']
                print(f"  ‚úì –ü—Ä–∏–º–µ–Ω–µ–Ω–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å: {quality_setting['formality']}")

            # –î–æ–±–∞–≤–ª—è–µ–º –≥–ª–æ—Å—Å–∞—Ä–∏–π –µ—Å–ª–∏ –µ—Å—Ç—å
            if glossary_id and hasattr(self.translator, 'translate_document_from_filepath'):
                translation_params["glossary"] = glossary_id
                print(f"  ‚úì –ì–ª–æ—Å—Å–∞—Ä–∏–π –ø—Ä–∏–º–µ–Ω–µ–Ω")

            # –≠—Ç–∞–ø 1: –ü–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            print("\n‚è≥ –≠—Ç–∞–ø 1: –ü–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞...")
            self.translator.translate_document_from_filepath(**translation_params)

            elapsed_translation = time.time() - start_time
            print(f"  ‚úì –î–æ–∫—É–º–µ–Ω—Ç –ø–µ—Ä–µ–≤–µ–¥–µ–Ω –∑–∞ {elapsed_translation:.1f} —Å–µ–∫.")

            # –≠—Ç–∞–ø 2: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Write API –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å—Ç–∏–ª—è (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –≤–∫–ª—é—á–µ–Ω)
            if (quality_setting.get('use_write_api') and
                self.write_api_available and
                output_path.suffix.lower() == '.docx' and
                DOCX_AVAILABLE):

                print("\n‚è≥ –≠—Ç–∞–ø 2: –£–ª—É—á—à–µ–Ω–∏–µ —Å—Ç–∏–ª—è (Write API)...")

                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ DOCX
                    doc = Document(output_path)
                    improved_count = 0

                    for para in doc.paragraphs:
                        if para.text.strip() and len(para.text) > 50:
                            original_text = para.text
                            improved_text = self.apply_write_api_improvement(
                                original_text,
                                target_lang,
                                quality_setting.get('writing_style', 'academic')
                            )

                            if improved_text != original_text:
                                para.text = improved_text
                                improved_count += 1

                            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                            if PAUSE_BETWEEN_REQUESTS > 0:
                                time.sleep(PAUSE_BETWEEN_REQUESTS)

                    if improved_count > 0:
                        doc.save(output_path)
                        print(f"  ‚úì –£–ª—É—á—à–µ–Ω–æ {improved_count} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
                    else:
                        print("  ‚Ñπ –£–ª—É—á—à–µ–Ω–∏—è –Ω–µ —Ç—Ä–µ–±—É—é—Ç—Å—è")

                except Exception as e:
                    print(f"  ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏—è: {e}")

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.usage_stats["documents_translated"] += 1
            self.usage_stats["characters_processed"] += file_size  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ

            elapsed_total = time.time() - start_time

            print(f"\n{'='*60}")
            print(f"‚úÖ –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û")
            print(f"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è: {elapsed_total:.1f} —Å–µ–∫.")
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path.name}")
            print(f"{'='*60}\n")

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if PAUSE_BETWEEN_REQUESTS > 0:
                time.sleep(PAUSE_BETWEEN_REQUESTS)

            return True

        except deepl.DocumentTranslationException as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            self.usage_stats["errors_encountered"] += 1
            return False
        except deepl.QuotaExceededException:
            print("‚úó –ü—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ API")
            return False
        except Exception as e:
            print(f"‚úó –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            self.usage_stats["errors_encountered"] += 1
            import traceback
            traceback.print_exc()
            return False

    def translate_folder_enhanced(self, folder_path: Path, source_lang: str,
                                 target_lang: str, quality_setting: Dict,
                                 glossary_id: Optional[str] = None):
        """–ü–µ—Ä–µ–≤–æ–¥ –ø–∞–ø–∫–∏ —Å —Ñ–∞–π–ª–∞–º–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –æ–ø—Ü–∏—è–º–∏"""

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–æ–≤
        output_folder = folder_path.parent / f"{folder_path.name}_translated_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        print(f"\nüìÅ –°–æ–∑–¥–∞—é –ø–∞–ø–∫—É –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–æ–≤: {output_folder}")
        try:
            output_folder.mkdir(exist_ok=True)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø–∞–ø–∫–∏: {e}")
            return

        # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤
        files_found = []
        for ext in SUPPORTED_EXTENSIONS:
            files_found.extend(folder_path.rglob(f"*{ext}"))

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤
        files_to_translate = []
        for f in files_found:
            if f.is_file() and not f.name.startswith('~$'):
                if not any(suffix in f.stem for suffix in ['_translated_', '_to_']):
                    files_to_translate.append(f)

        if not files_to_translate:
            print(f"\n‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ {SUPPORTED_EXTENSIONS} –≤ –ø–∞–ø–∫–µ.")
            return

        print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞: {len(files_to_translate)}")
        print(f"üíæ –ü–µ—Ä–µ–≤–æ–¥—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_folder}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
        print("\nüìã –§–∞–π–ª—ã –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞:")
        for i, file in enumerate(files_to_translate[:10], 1):
            print(f"  {i}. {file.name}")
        if len(files_to_translate) > 10:
            print(f"  ... –∏ –µ—â–µ {len(files_to_translate) - 10} —Ñ–∞–π–ª–æ–≤")

        # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
        confirm = input("\n‚ñ∂ –ù–∞—á–∞—Ç—å –ø–µ—Ä–µ–≤–æ–¥? (y/n): ")
        if confirm.lower() != 'y':
            print("–û—Ç–º–µ–Ω–µ–Ω–æ.")
            return

        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
        success_count = 0
        error_count = 0
        skipped_count = 0

        print(f"\nüöÄ –ù–∞—á–∏–Ω–∞—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥...\n")
        print("=" * 60)

        for i, file_path in enumerate(files_to_translate, 1):
            # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
            try:
                relative_path = file_path.relative_to(folder_path)
            except ValueError:
                relative_path = file_path.name

            # –°–æ–∑–¥–∞–µ–º –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            output_path = output_folder / relative_path.parent / f"{file_path.stem}{TRANSLATION_SUFFIX}_{target_lang.lower()}{file_path.suffix}"

            # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–ø–∞–ø–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            output_path.parent.mkdir(parents=True, exist_ok=True)

            print(f"\n[{i}/{len(files_to_translate)}] üìÑ {relative_path}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ñ–∞–π–ª
            if output_path.exists():
                print("  ‚è≠ –ü—Ä–æ–ø—É—Å–∫: —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                skipped_count += 1
                continue

            # –ü–µ—Ä–µ–≤–æ–¥–∏–º —Ñ–∞–π–ª
            if self.translate_document_enhanced(
                file_path, output_path, source_lang, target_lang,
                quality_setting, glossary_id
            ):
                success_count += 1
            else:
                error_count += 1

        # –ò—Ç–æ–≥–∏
        print("\n" + "=" * 60)
        print("üìä –ò–¢–û–ì–ò:")
        print(f"  üìÅ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(files_to_translate)}")
        print(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–æ: {success_count}")
        print(f"  ‚ùå –û—à–∏–±–æ–∫: {error_count}")
        print(f"  ‚è≠ –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}")
        print(f"  üíæ –ü–µ—Ä–µ–≤–æ–¥—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_folder}")
        print("=" * 60)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self._print_usage_stats()

    def _print_usage_stats(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏:")
        print(f"  –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–æ: {self.usage_stats['documents_translated']}")
        print(f"  –°–∏–º–≤–æ–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: ~{self.usage_stats['characters_processed']:,}")
        print(f"  –û—à–∏–±–æ–∫ –≤—Å—Ç—Ä–µ—á–µ–Ω–æ: {self.usage_stats['errors_encountered']}")

        cache_stats = self.cache.get_stats()
        if cache_stats['cache_hits'] > 0:
            hit_rate = cache_stats['cache_hits'] / (cache_stats['cache_hits'] + cache_stats['cache_misses']) * 100
            print(f"  –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫—ç—à–∞: {hit_rate:.1f}%")
            print(f"  –°—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤: {cache_stats['total_chars_saved']:,}")


def get_language_choice() -> Tuple[str, str]:
    """–í—ã–±–æ—Ä –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π)"""
    print("\nüåç –í—ã–±–µ—Ä–∏—Ç–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞:")
    for key, value in LANGUAGE_OPTIONS.items():
        print(f"  {key}: {value['name']}")

    while True:
        choice = input(f"\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1-{len(LANGUAGE_OPTIONS)}): ").strip()
        if choice in LANGUAGE_OPTIONS:
            lang = LANGUAGE_OPTIONS[choice]
            return lang['source'], lang['target']
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")


def get_quality_choice() -> Dict:
    """–í—ã–±–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞"""
    print("\n‚öô –í—ã–±–µ—Ä–∏—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:")
    for key, value in QUALITY_SETTINGS.items():
        suffix = " (—Ç—Ä–µ–±—É–µ—Ç Pro)" if value.get('use_write_api') else ""
        print(f"  {key}: {value['name']}{suffix}")

    while True:
        choice = input(f"\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1-{len(QUALITY_SETTINGS)}): ").strip()
        if choice in QUALITY_SETTINGS:
            return QUALITY_SETTINGS[choice]
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")


def manage_glossaries(translator: EnhancedTranslator) -> Optional[str]:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è–º–∏"""
    print("\nüìö –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è–º–∏")
    print("  1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –≥–ª–æ—Å—Å–∞—Ä–∏–π")
    print("  2. –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –≥–ª–æ—Å—Å–∞—Ä–∏–π –≤—Ä—É—á–Ω—É—é")
    print("  3. –°–æ–∑–¥–∞—Ç—å –Ω–∞—É—á–Ω—ã–π –≥–ª–æ—Å—Å–∞—Ä–∏–π –∏–∑ —à–∞–±–ª–æ–Ω–∞")
    print("  4. –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≥–ª–æ—Å—Å–∞—Ä–∏–π –∏–∑ —Ñ–∞–π–ª–∞")
    print("  5. –ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–ª–æ—Å—Å–∞—Ä–∏–π")

    choice = input("\n–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ (1-5): ").strip()

    if choice == '1':
        # –°–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≥–ª–æ—Å—Å–∞—Ä–∏–µ–≤
        glossaries = translator.glossary_manager.list_glossaries()
        if not glossaries:
            print("‚ùå –ù–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –≥–ª–æ—Å—Å–∞—Ä–∏–µ–≤")
            return None

        print("\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –≥–ª–æ—Å—Å–∞—Ä–∏–∏:")
        for i, g in enumerate(glossaries, 1):
            print(f"  {i}. {g['name']} ({g['source_lang']} -> {g['target_lang']})")
            print(f"     –ó–∞–ø–∏—Å–µ–π: {len(g['entries'])}")

        idx = input("\n–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–º–µ—Ä –≥–ª–æ—Å—Å–∞—Ä–∏—è: ").strip()
        try:
            selected = glossaries[int(idx) - 1]
            return selected.get('id')
        except:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")
            return None

    elif choice == '2':
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –≥–ª–æ—Å—Å–∞—Ä–∏—è
        name = input("–í–≤–µ–¥–∏—Ç–µ –∏–º—è –≥–ª–æ—Å—Å–∞—Ä–∏—è: ").strip()
        source = input("–ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, RU): ").strip().upper()
        target = input("–¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, EN-US): ").strip().upper()

        print("\n–í–≤–µ–¥–∏—Ç–µ —Ç–µ—Ä–º–∏–Ω—ã (—Ñ–æ—Ä–º–∞—Ç: –∏—Å—Ö–æ–¥–Ω—ã–π_—Ç–µ—Ä–º–∏–Ω -> –ø–µ—Ä–µ–≤–æ–¥)")
        print("–ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è")

        entries = {}
        while True:
            entry = input("> ").strip()
            if not entry:
                break

            parts = entry.split('->')
            if len(parts) == 2:
                entries[parts[0].strip()] = parts[1].strip()
            else:
                print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: —Ç–µ—Ä–º–∏–Ω -> –ø–µ—Ä–µ–≤–æ–¥")

        if entries:
            glossary_id = translator.glossary_manager.create_glossary(
                name, source, target, entries
            )
            return glossary_id

    elif choice == '3':
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—É—á–Ω–æ–≥–æ –≥–ª–æ—Å—Å–∞—Ä–∏—è –∏–∑ —à–∞–±–ª–æ–Ω–∞
        print("\nüî¨ –î–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏:")
        for idx, (key, name) in enumerate(SCIENTIFIC_FIELDS.items(), 1):
            print(f"  {idx}. {name} ({key})")

        field_choice = input(f"\n–í—ã–±–µ—Ä–∏—Ç–µ –æ–±–ª–∞—Å—Ç—å (1-{len(SCIENTIFIC_FIELDS)}): ").strip()
        try:
            field_key = list(SCIENTIFIC_FIELDS.keys())[int(field_choice) - 1]
            print(f"\n‚úì –í—ã–±—Ä–∞–Ω–∞ –æ–±–ª–∞—Å—Ç—å: {SCIENTIFIC_FIELDS[field_key]}")

            source = input("–ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫ (EN): ").strip().upper() or "EN"
            target = input("–¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫ (RU): ").strip().upper() or "RU"

            glossary_id = translator.glossary_manager.create_scientific_glossary(
                field_key, source, target
            )
            return glossary_id
        except (ValueError, IndexError):
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")
            return None

    elif choice == '4':
        # –ò–º–ø–æ—Ä—Ç –∏–∑ —Ñ–∞–π–ª–∞
        file_path = input("–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –≥–ª–æ—Å—Å–∞—Ä–∏—è: ").strip().strip('"\'')
        file_path = Path(file_path)

        if not file_path.exists():
            print("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None

        entries = translator.glossary_manager.load_glossary_from_file(file_path)
        if entries:
            name = input("–ò–º—è –≥–ª–æ—Å—Å–∞—Ä–∏—è: ").strip()
            source = input("–ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫: ").strip().upper()
            target = input("–¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫: ").strip().upper()

            glossary_id = translator.glossary_manager.create_glossary(
                name, source, target, entries
            )
            return glossary_id

    return None


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 60)
    print("üöÄ DeepL Enhanced Translator")
    print("üìö –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –Ω–∞—É—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print("=" * 60)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    translator = EnhancedTranslator()
    if not translator.initialize():
        sys.exit(1)

    # –ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é
    while True:
        print("\nüìã –ì–õ–ê–í–ù–û–ï –ú–ï–ù–Æ:")
        print("  1. –ü–µ—Ä–µ–≤–µ—Å—Ç–∏ –æ–¥–∏–Ω —Ñ–∞–π–ª")
        print("  2. –ü–µ—Ä–µ–≤–µ—Å—Ç–∏ –ø–∞–ø–∫—É —Å —Ñ–∞–π–ª–∞–º–∏")
        print("  3. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è–º–∏")
        print("  4. –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –ø–µ—Ä–µ–≤–æ–¥–æ–≤")
        print("  5. –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É")
        print("  6. –í—ã—Ö–æ–¥")

        choice = input("\n–í–∞—à –≤—ã–±–æ—Ä (1-6): ").strip()

        if choice == '1':
            # –ü–µ—Ä–µ–≤–æ–¥ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            file_path_str = input("\nüìÑ –í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É: ").strip().strip('"\'')

            try:
                file_path = Path(file_path_str).resolve()
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—É—Ç–∏: {e}")
                continue

            if not file_path.exists():
                print(f"‚ùå –ü—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {file_path}")
                continue

            if not file_path.is_file():
                print(f"‚ùå –≠—Ç–æ –Ω–µ —Ñ–∞–π–ª: {file_path}")
                continue

            print(f"‚úÖ –§–∞–π–ª –Ω–∞–π–¥–µ–Ω: {file_path.name}")
            print(f"üìé –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ: {file_path.suffix.lower()}")

            if file_path.suffix.lower() not in SUPPORTED_EXTENSIONS:
                print(f"‚ùå –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ {SUPPORTED_EXTENSIONS}")
                continue

            # –í—ã–±–æ—Ä —è–∑—ã–∫–æ–≤
            source_lang, target_lang = get_language_choice()

            # –í—ã–±–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞
            quality_setting = get_quality_choice()

            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –≤—ã–±–æ—Ä –≥–ª–æ—Å—Å–∞—Ä–∏—è
            glossary_id = None
            use_glossary = input("\nüìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–ª–æ—Å—Å–∞—Ä–∏–π? (y/n): ").strip().lower()
            if use_glossary == 'y':
                glossary_id = manage_glossaries(translator)

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            output_path = file_path.with_name(
                f"{file_path.stem}{TRANSLATION_SUFFIX}_{target_lang.lower()}{file_path.suffix}"
            )

            # –ü–µ—Ä–µ–≤–æ–¥
            translator.translate_document_enhanced(
                file_path, output_path, source_lang, target_lang,
                quality_setting, glossary_id
            )

        elif choice == '2':
            # –ü–µ—Ä–µ–≤–æ–¥ –ø–∞–ø–∫–∏
            folder_path_str = input("\nüìÅ –í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ: ").strip().strip('"\'')
            folder_path = Path(folder_path_str)

            if not folder_path.is_dir():
                print("‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
                continue

            # –í—ã–±–æ—Ä —è–∑—ã–∫–æ–≤
            source_lang, target_lang = get_language_choice()

            # –í—ã–±–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞
            quality_setting = get_quality_choice()

            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –≤—ã–±–æ—Ä –≥–ª–æ—Å—Å–∞—Ä–∏—è
            glossary_id = None
            use_glossary = input("\nüìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–ª–æ—Å—Å–∞—Ä–∏–π? (y/n): ").strip().lower()
            if use_glossary == 'y':
                glossary_id = manage_glossaries(translator)

            # –ü–µ—Ä–µ–≤–æ–¥ –ø–∞–ø–∫–∏
            translator.translate_folder_enhanced(
                folder_path, source_lang, target_lang,
                quality_setting, glossary_id
            )

        elif choice == '3':
            # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è–º–∏
            manage_glossaries(translator)

        elif choice == '4':
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
            confirm = input("\n‚ö† –û—á–∏—Å—Ç–∏—Ç—å –≤–µ—Å—å –∫—ç—à –ø–µ—Ä–µ–≤–æ–¥–æ–≤? (y/n): ").strip().lower()
            if confirm == 'y':
                translator.cache.clear()
                print("‚úÖ –ö—ç—à –æ—á–∏—â–µ–Ω")

        elif choice == '5':
            # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            translator._print_usage_stats()

            # –¢–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API
            try:
                usage = translator.translator.get_usage()
                if usage.character.count and usage.character.limit:
                    remaining = usage.character.limit - usage.character.count
                    percentage = (usage.character.count / usage.character.limit) * 100
                    print(f"\nüìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API:")
                    print(f"  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {usage.character.count:,} —Å–∏–º–≤–æ–ª–æ–≤")
                    print(f"  –õ–∏–º–∏—Ç: {usage.character.limit:,} —Å–∏–º–≤–æ–ª–æ–≤")
                    print(f"  –û—Å—Ç–∞–ª–æ—Å—å: {remaining:,} —Å–∏–º–≤–æ–ª–æ–≤ ({100-percentage:.1f}%)")
            except:
                pass

        elif choice == '6':
            print("\nüëã –í—ã—Ö–æ–¥ –∏–∑ –ø—Ä–æ–≥—Ä–∞–º–º—ã.")
            break
        else:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä.")

    print("\n‚ú® –°–ø–∞—Å–∏–±–æ –∑–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ DeepL Enhanced Translator!")

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if translator.usage_stats["documents_translated"] > 0:
        translator._print_usage_stats()


if __name__ == "__main__":
    main()